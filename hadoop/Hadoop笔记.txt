
=======================================================================================================
切片流程
Input(读入数据集)-->InputFormat(切片)-->shuffle-->OutputFormat-->Output(输出结果集)

数据切片与MapTask并行度机制？
1、一个Job的Map阶段并行度是由客户端在提交Job时的切片数决定。
2、每一个Split切片分配一个MapTask并行实例处理。
3、默认情况下，切片大小=BlockSize。
4、切片时不考虑数据集整体，而是逐个针对每一个文件单独切片。

切片的规则（切片的在具体执行是依靠InputFormat这个类进行的）:
Math.max(minSize, Math.min(maxSize, blockSize));
切片的最小值，切片的最大值，块大小
取中间那个值

当文件的大小大于128Mb*1.1的时候，开始切片。
当文件的大小小于128Mb*1.1的时候，把该文件当成一个整体。

CombineTextInputFormat切片方式：先按照4Mb进行文件分块，之后把分成的块逐个累加，没当刚大于4mb的时候，切分为一片
小于4Mb为一块，大于4Mb的文件，小于4*2mb的文件，先切一个4mb的块，在将剩下的平均切成两块。

InputFormat      切片方法            kv方法
Text             FIF的切片方法        LineRecordReader
Text             FIF的切片方法        KeyValueLineRecordReader
NLine            自定义，n行一片       LineRecordReader
CombineText      自定义               CombineFIleRecordReader
FixedLength      FIF的切片方法         FixedLengthRecordReader
SequenceFile     FiF的切片方法         SequenceFileRecordReader

MapReduce的详细工作流程
1、待处理文本 /user/input
2、客户端submit()前，获取待处理的数据的信息，然后根据参数配置，形成一个任务分配的规划。
3、提交信息（Job.split、wc.jar、job.xml等）
4、计算MapTask的数量
5、默认TextInputFormat
6、逻辑运算
7、向环形缓冲区写入kv数据
8、分区、排序，利用归并排序进行全排序
9、溢出到文件（分区且区内有序）
10、Merge归并排序。

区内进行快排

MapTask = 分片数（所谓分区：指明数据应该去往哪个MapTask）
Reduce = 需求（可以自己指定，每个reduceTask对应一个分区文件）

shuffle机制:
1、map--写入kv数据到缓冲区；
    具体过程：先将数据写入到数据缓冲区（默认为100Mb），此时的数据已经被逻辑分区为（k，v，pId），
    再进行二次排序（先按照分区号排序，内部再按照key排序）
    当数据量大于缓冲区的时候，将数据溢写到磁盘，此时需要按照分区号进行物理上的分区溢写；
    combiner是为了减少io。
    数据落盘之前，假如开启了combiner，则将数据输入到combiner进行一次合并，再进行真正落盘；
    combiner为可选流程，主要按照分区进行进行归并排序，之后在按照分区进行合并，压缩之后写入到磁盘。

假设有n个mapTask,每个reduceTask都会处理所有mapTask相应分区的数据。

Reduce--输出文件，将数锯copy到内存缓冲区，内存不够则溢写到磁盘，最后进行归并排序合成整个文件，
文件特点：按照相同的key有序排列，分组之后出入到reduce方法中去

环形内存缓冲区一边存储kv，以边存储索引，达到80%后就开始溢写，当数据再来了的时候，在10%的位置开始写，将80%中的数据溢写到磁盘。
溢写的时候发生二次排序（快排），分区排序和key排序，该排序的方法发生在内存环形缓冲区中，排序的时候不交换数据，仅交换索引。



2、partition--sort--内存不够，溢出刷新缓冲区数据到磁盘，先进行分区，再进行快速排序；
3、combiner--按照分区进行归并排序，合并分区并排序；
4、得到分区输出结果；
copy--copy到内存中，内存不够则溢出数据到磁盘，对每个map来的数据进行归并排序--按照相同的key分组--交给reducer方法

Partitioner的分区号，必须从0开始。

！！！ reduce输入原理
分区：
分组:针对有序的分区数据
归并：打多个文件合并成一个文件





分区的意义：告诉某一条数据应该被哪个Task处理

Map到Reduce的过程经过强制性的全排序。
 
combiner作用：减少磁盘Map阶段的局部IO

排序和分組的關係不一樣，先排序后分組

假如分組規則和排序規則不一樣，排序規則要比分組規則的粒度更細


在同一個節點中，數據的傳遞不是按照對象傳遞的，是序列化之後再傳遞，反序列化后接受

整個MR過程的數據傳遞都需要經過序列化

=======================================================================================================
ReduceJoin思路如下：
1、join字段相同的数据进入同一组。
2、join的工作在reduce中完成。
因为reduce阶段有数据的汇总，所以reduce可以完成join。
而map适用于一张表十分小，一张表很大的场景，需要把小表完全的缓存中内存。

思考：在Reduce端处理过多的表，非常容器出现数据倾斜，怎么办？
在map端完成了join，则不在需要reduce，没有reduce则没有shuffle。没有shuffle就没有数据倾斜，处理速度就会很快。
数据经过shuffle后会重新进行数据分配，进而产生数据倾斜。
=====================================================================================================
小表的定义：
小表的大小控制在15Mb之下，
在hive中小表的大小控制在25Mb之下。
=====================================================================================================
数据清洗--ETL
市面上有专用的工具，该代码主要演示计数器的使用
====================================================================================================
Hadoop的HA

===================================================
hdfs最大的瓶颈就是NameNode的内存

一个128G内存的NameNode，可以存大约109Pb的数据

128G * 1024 * 1024 * 1024 byte 换算成字节
/ 50 byte 因为一个块的元数据信息大约占50字节，得到可以存储多少块
* 128 mb  倘若一个块大小为128 mb ，最大可能为512 mb
/1024 /1024 /1024 换算成PB，
得到当块大小为128mb时，可以存储109 pb
得到当块大小为512mb时，可以存储436 pb

hdfs federation 是为了解决NameNode内存不够的问题。

1 kB = 1024 B (kB - kilobajt) 千zhidao
1 MB = 1024 kB (MB - megabajt) 兆
1 GB = 1024 MB (GB - gigabajt) 吉回
1 TB = 1024 GB (TB - terabajt) 太
1 PB = 1024 TB (PB - petabajt) 拍
1 EB = 1024 PB (EB - eksabajt) 艾答
1 ZB = 1024 EB (ZB - zettabajt) 皆
1 YB = 1024 ZB (YB - jottabajt) 佑
1 BB = 1024 JB (BB - brontobajt)
=====================================================================================================
压缩的基本准则：
IO密集型的job，多用压缩
计算密集型的job，少用压缩

对比优缺点，决定采用以下两种
lzo和snappy
可以使用configuration对象配置压缩开启和压缩方式
=====================================================================================================
yarn资源管理器
yarn本质是一个资源池
RM：总监，资源管理器
NM：员工，节点管理器
ApplicationMaster：job
Container：资源池里面抠出来一块资源
NodeManager通关不断的开关Container来管理节点资源

资源很紧张的时候，需要对资源进行精细管理的时候需要使用公平调度器。
容量调度器：多个FIFO队列，分别分享指定额度的资源。
公平调度器：层级管理任务。
=====================================================================================================
任务推测执行
防止某个节点上任务运行过慢，拖慢整个任务的处理速度。yarn监控到运行很慢的任务，则开启备份任务执行同样的任务，最终获取执行最快的任务的结果，杀死另外的。
执行推测任务的前提条件：
1、每个Task只能由一个备份任务。
2、当前Job已完成的Task必须不小于0.05%。
3、开启推测执行参数设置。mapred-site.xml文件中默认是打开的。
不能开启推测执行的情况：
1、任务间存在严重的负载倾斜。
2、特殊任务，任务想数据库写数据。
注意：
1、MR中指定了作业中最大备份任务的数量。
2、推测执行总是选择差值最大的任务，并为之启动备份任务。
3、在集群在原紧缺的情况下，应合理利用该机制。

===================================================================================================
MapReduce程序效率的瓶颈在于两点：
1、计算机性能。
2、IO操作优化。
    1、数据倾斜。
    2、Map和Reduce数设置不合理。
    3、Map运行时间太长，导致Reduce等待很久。
    4、小文件过多。
    5、大量的不可分块的超大文件。
    6、Spill次数过多。
    7、merge次数过多。
================================================================================================
MapReduce优化方法：
一般从六个维度考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。

一、数据输入（1、不用小文件 2、tar包 3、开启JVM重用 4、CombineTaskFormat）
JVM重用讲解：执行完Task之后不会重启，会继续执行Task
缺点是：JVM得不到释放，会慢慢减小内存

HDFS小文件解决方案：
1、Hadoop Archive
   是一个高效的将小文件放入到hdfs块中的文件存档工具，它能够将许多小文件打包成一个
   HAR文件，这样就减少了NamaNode的内存使用。
2、Sequence File
    SequenceFile由一些列的二进制key/value组成，如果key为文件名，value为文件内从，
    则可以将大批小文件合并成一个大文件。
3、CombineFileInputFormat
   是一种新的InputFormat，用于将多个文件合并成一个单独的Split，另外，它会考虑存储位置。
4、开启JVM重用
   对于大量小文件Job，可以开启JVM重用会减少45%运行时间。













